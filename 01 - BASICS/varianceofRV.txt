So what we have seen so far is the following we have looked at what was a random variable,
we defined what is a random variable and then we looked at the types of random variable
namely we said that there are two main types of a random variable we can classify them
as discrete random variable and continuous random variables.
Then we looked at when we focus our, we restricted our focus only to discrete random variables,
in discrete random variables we defined what was a probability mass function in the sense
that you are interested in knowing about a distribution of a random variable, hence we
introduced the notion of a probability mass function.
The probability mass function tells us about the probability distribution of a random variable
and the cumulative distribution function helps us to know what is the overall distribution
of the random variable given a real line.
So, we also looked at the graphs of both the probability mass function and the cumulative
distribution function.
Then we went out to introduce an extremely important notion of a random variable namely
the expectation of a random variable, we focused on properties of a random variable and then
after this we looked at how we you can compute the expectation of functions of a random variable.
Today we are going to focus on another important summary of a random variable which we refer
to as a variance of a random variable.
So, let us first of all motivate the need for such a measure.
So we saw that we also realize that we can interpret the expected value as a long-run
average, so in other words the way we introduced the expected value of a random variable was
as a weighted average of possible values the random variable can take, in other words if
X can take values x1 x2 xn I am assuming finitely many with probability X equal to xi equal
to P1 P2 Pn these are I assume P1 P2 Pn are the weights given to the values, then I know
that the expectation of X can be written as P1x1 plus P2x2 plus Pnxn, in other words it
is X is taking possible values x1 x2 to xn with weights P1 P2 to Pn then expectation
02:57
of X can be expressed as a weighted average of the possible values of random variable.
Now, let us look at an example where I have three random variables.
So, what are these three random variables, X is a random variable with probability 1,
Y is a random variable which takes the values minus 2 with probability half and plus 2 with
probability half and Z is a random variable which takes the value minus 20 with probability
half and plus 20 with probability half.
Now if you compute X, I can write X takes the value 0 with probability X equal to i
equal to 1 which gives me the expectation of X is 0 into 1 which is equal to 0.
Similarly, I know Y takes the value minus 2 and plus 2 with probability Y equal to i
equal to half and half giving me expectation of Y equal to minus 2 into half plus 2 into
half which is again a 0.
Z takes the value minus 20 and 20 with probability Z is equal to i is again half and half which
gives me expectation of Z, which is equal to minus 20 into half plus 20 into half which
is again 0.
So, I can see that expectation of X equals expectation of Y equals expectation of Z,
so this expected value however what we notice is though the expectation is same the distribution
of these random variables with the terms of the values they take is different.
In other words, the expected value of a random variable does not tell us anything about the
spread or the variation.
Hence, I need a measure for the spread.
What I can notice here although is the spread of Z is greater than the spread of Y which
is greater than the spread of X.
The expected value although was the same expected value of X equal to expected value of Y equal
to expected value of Z which is equal to 0.
Hence, I need a measure of spread and just as what we did in a descriptive statistics
module mean gave us a measure of central tendency variance gave us a measure of spread.
So, we are going to define the variance of a random variable, which is a measure of spread.
So, what how do I define the variance of a random variable?
So, let me denote the expected value of a random variable by the Greek alphabet mu that
is expectation of X is equal to mu, X is my random variable I see how far is X from mu,
I take the square of the difference and the expected value of the squared difference and
that is what I am going to define as my variance of X.
So, the if X is a random variable with expected value mu then the variance of X is defined
by expectation of X minus mu whole square.
In other words, the variance of a random variable defines the square of the difference of the
random variable from its mean on an average.
I repeat, the variance of a random variable measures the difference of the random variable
from its mean, so that is X minus mu, it measures the square of the difference which is X minus
mu whole square on an average is expectation of X minus mu whole square.
So, when I talk about X minus mu whole square the question is can I computationally have
a more convenient formula to compute the variance of a random variable?
I know variance of a random variable is expectation of X minus mu whole square.
Now, let us look at X minus mu whole square, I know X minus mu whole square is X square
plus mu square minus 2 times mu into X.
Now, recall from my expectation or properties of expectation, expectation of aX plus b where
a and b are constants as a times expectation of X plus b.
So, mu is a constant here, so if I am looking at expectation of X minus mu whole square,
I know this is equal to expectation of X square plus mu square minus 2 times mu of X which
I can write it as expectation of X square plus expectation of mu square is going to
be mu square minus 2 mu times expectation of X, I know expectation of X is a mu, hence
this reduces to expectation of X square plus mu square minus 2 mu into mu which gives me
expectation of X square minus mu square.
So, the computational formula for X is a very simple formula which is the same as variance
of X is expectation of X square minus mu square, where mu is expectation of X.
So, now let us apply this formula to compute the variance of the random variables which
we have discussed earlier.
Let us begin with the role of a fair dice and I am rolling it once I know the sample
space is 1, 2, 3, 4, 5, 6, X is the outcome of the roll, so my X takes values 1, 2, 3,
4, 5, 6, I am going to apply the formula the computational formula variance of X is expectation
of X square minus expectation of X whole square or expectation of X square minus mu square.
That is the formula I am going to apply.
So, if X takes the value 1, 2, 3, 4, 5, 6, I know X square takes the value 1 square,
2 square is a 4, 3 square is a 9, 4 square is a 16, 5 square is a 25 and 6 square is
a 36.
Now, if X takes the value 1, X square takes the value 1 with the same probability.
Similarly, if X takes the value 2, X square takes the value 4 with probability 1 by 6,
X takes the value 3, X square takes the value 9 with probability 1 by 6 and so forth X takes
the value i, X square takes the value i square with probability 1 by 6.
We have already computed expectation of X and we notice that expectation of X was 3.5.
So, what is expectation of X square?
X square takes the value 1 with probability 1 by 6, 4 with probability 1 by 6, 9 with
probability 1 by 6, 16 with probability 1 by 6, 25 with probability 1 by 6 and 36 with
probability 1 by 6, which gives me expectation of X square is 15.167, if expectation of X
is 3.5 expectation of X whole square is 3.5 square and hence my variance of X is 15.167
which is expectation of X square minus mu square which is 2.917.
Similarly, let us look at the example of rolling a dice twice, again we know that X takes values
2, 3, 4 up to 12, hence X square takes the values 4, 9, 16, 25 up to 144, again the probability
with which X takes the value 2 is same as the probability with which X takes the value
X square takes the value 4 which is equal to 1 by 6 in this case and hence my expectation
of X we have already computed was 7, I can verify that expectation of X square is 54.833
giving me variance of X is 5.833.
So, we notice the following that in the earlier case when X was a roll of a dice it took the
value 1, 2, 3, 4, 5, 6, it had equal probability and now you can see that the mean was around
3.5 this was somehow balancing it, here the mean is at 7 and I can see that the variance
in this case is 5.833 because it is taking a value from 2 to 12, so the variability is
more, here it is taking the value 1 to 6 and it had a variance of about 2.91.
Now, let us apply the formula to tossing a coin thrice, again I know that X is a random
variable which takes the values, again I know X is a random variable which takes the values
0, 1, 2, 3, hence X square is equal to 0, 1, 4 and 9 with the same probabilities, so
I can verify that expectation of X, I already have checked.
Expectation of X was 3 by 2, expectation of X square is going to be 0 into 1 by 8 plus
1 into 3 by 8 plus 4 into 3 by 8 plus 9 into 1 by 8 that would be my expectation of X square
which I can verify that to be 24 by 8, which is 3.
Hence, my variance of X is 3 minus 2.25 which is 0.5, because this is 1.5, 1.5 square is
2.25 variance of X is 3 minus 2.25 which is a 0.75.
Again, you can see that the value of X is between 0 and 3, so I had when X took the
value 1, 2, 3, 4, 5, 6 with equal probability I had a variance of 2.91, X took the value
2, 3, 4 up to 12 with varied probabilities, then I had 5.83, now X is again taking the
value 0, 1, 2, 3 with probability 1 by 8, 3 by 8, 3 by 8 and 1 by 8, my variance is
very less which is 0.75.
So, now let us look at the specific random variables which we have already defined recall
we defined a Bernoulli random variable as that random variable which takes only 2 values
for convenience I assumed it took the value 0 and 1 with probability X taking value 0
as 1 minus p and value 1 equal to p, if X takes the value 0 and 1, X square also takes
the value 0 and 1 with the same probability is 1 minus p and p.
We verified that expectation of X is 0 into 1 minus p plus 1 into p, which gave me the
expectation of X equal to p, expectation of X square is also 0 into 1 minus p plus 1 into
p which is p, hence the variance equal to expectation of X square which is equal to
p minus expectation of X whole square which is minus p square, which I can write as p
into 1 minus p.
Hence the variance of a Bernoulli random variable is p into 1 minus p.
So, I can refer to this as if x is a Bernoulli random variable with parameter p, then the
expectation of X is p, the variance of X is p into 1 minus p.
Let us, look at that case of a discrete uniform random variable, again I know I call a random
variable to be a uniformly distributed random variable if X takes the value 1, 2 up to n
with probability 1 by n, 1 by n, 1 by n, again an example of this is my roll of a single
dice where my n was 6 and I knew that probability X equal to each value is 1 by 6, 1 by 6, 1
by 6.
So, if X takes these values then X square takes the value 1, 4 up to n square, the probabilities
remain the same.
We have already seen the expectation of X equal to 1 into 1 by n plus 2 into 1 by n
plus n into 1 by n which we showed was 1 by n into the sum of the first n natural numbers
which we further saw that this was n into n plus 1 by 2 into 1 by n which gave me n
plus 1 by 2.
So, I already verified that expectation of X equal to n plus 1 by 2, now when n equal
to 6, I know that this is equal to 7 by 2, which is 3.5 and this 3.5 actually matches
with my expectation of a single roll of a dice that is 3.5, recall, I got a 3.5 when
I found out the expectation of rolling a dice 1 was 3.5.
Now, from here remember the variance was 2.91, so let us go and check for a discrete random
variable.
So, again when I go and do my check for a discrete random variable I see that what is
expectation of X square in this case.
Now, if I look at X square X square takes the value 1 plus 4 plus n square so I know
expectation of X square, I can write it as 1 by so expectation of x square I can write
it as 1 by n into 1 plus 1 by n into 4 plus 1 by n into n square, so I get 1 by n plus
1 plus 2 square plus 3 square plus n square, the term within the brackets is nothing but
the sum of the squares of the first n natural numbers.
And we can verify and we already know that the expectation of n square is nothing but
n plus 1 into 2 n plus 1 by 6, because the sum of squares is n into n plus 1 into 2 n
plus 1 by 6.
Now, the variance of X is going to be n plus 1 into 2 n plus 1 by 6 minus n plus 1 whole
square by 4, this is expectation of X square, this is expectation of X whole square and
I can show that this would be this is going to be a 2 n square.
plus 3 n plus 1 minus n square plus 1 plus 2 n by 4 by 6 and I can verify that this is
going to be 2 n square plus 12 n minus 12 n cancel out plus 4 minus 6, which is equal
to minus 2 by 24 which gives me the fact that variance of X is n square minus 1 by 12.
So, we computed the variance of very well-known distributions namely the Bernoulli distribution
and the discrete uniform distribution.
We will be looking at a few more distributions later, but for now in summary, we introduced
the notion of a variance, the notion of a variance of a random variable captures the
measure of spread, hence it is an important measure and we also computed through the computational
formula and the computational formula is variance of X is expectation of X square minus the
expectation of X whole square which is the same as expectation of X square minus mu square,
we applied this formula to compute the variance of some well-known distributions and examples
that we have already seen.